{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from yaml import safe_load\n",
    "from os.path import join, exists, getsize\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "def collate_fn(data):\n",
    "    imgs, labels = zip(*data)\n",
    "    assert len(imgs) == len(labels)\n",
    " \n",
    "    for i, lb in enumerate(labels):\n",
    "        lb[:, 0] = i  # add target image index for build_targets()\n",
    "\n",
    "    labels = torch.cat(labels, 0)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "class URPC2020_DataSet(Dataset):\n",
    "    def __init__(self, data_yaml_path=\"/home/szt/projects/ultralytics/hg.yaml\", type=\"train\", transforms=None):\n",
    "        self.data_yaml_path = data_yaml_path\n",
    "        self.transforms = transforms\n",
    "        with open(self.data_yaml_path, 'r') as file:\n",
    "            self.data_dict = safe_load(file)\n",
    "        self.nc = self.data_dict['nc']\n",
    "        self.root_path = self.data_dict[\"path\"]\n",
    "        self.classes = self.data_dict[\"names\"]\n",
    "\n",
    "        if(type == \"train\"):\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"train\"])\n",
    "        elif type == \"val\":\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"val\"])\n",
    "        else:\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"test\"])\n",
    "\n",
    "        self.labels_path = f\"{self.root_path}/{type}/labels\"\n",
    "        labels = listdir(self.labels_path)\n",
    "        self.labels = self.filter_labels(labels)\n",
    "    \n",
    "    def filter_labels(self, labels):\n",
    "        # filter empty labels\n",
    "        labels = [label for label in labels if getsize(join(self.labels_path, label)) != 0]\n",
    "        return labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = label[:-3] + \"jpeg\" #.jpg\n",
    "        image_path = f\"{self.images_path}/{image}\"\n",
    "        label_path = f\"{self.labels_path}/{label}\"\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        label = []\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                line_strip = line.strip().split(\" \") #index in batch, cls, bbox\n",
    "                label.append([int(0), int(line_strip[0]), float(line_strip[1]), float(line_strip[2]), float(line_strip[3]), float(line_strip[4])])\n",
    "        img = self.transforms(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def group_adjust(labels, batch_size):\n",
    "    # 创建一个字典，按照第一列的值进行分组\n",
    "    grouped_labels = [[] for i in range(batch_size)]\n",
    "    for row in iter(labels):\n",
    "        i = int(row[0].item())\n",
    "        grouped_labels[i].append(row[1:])\n",
    "    try:\n",
    "        grouped_labels = [torch.stack(group) for group in grouped_labels]\n",
    "    finally:\n",
    "        return grouped_labels\n",
    "\n",
    "\n",
    "def build_dataset(yaml_path=\"/home/szt/projects/ultralytics/hg.yaml\", batch_size=16, data_transform=None):\n",
    "    if(data_transform is None):\n",
    "        data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    # Create a custom dataset\n",
    "    dataset = URPC2020_DataSet(yaml_path, transforms=data_transform)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=4,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([16, 6])\n",
      "tensor([0.0000, 2.0000, 0.5480, 0.4997, 0.0363, 0.0460])\n"
     ]
    }
   ],
   "source": [
    "data_loader = build_dataset()\n",
    "for i, (imgs, labels) in enumerate(data_loader):\n",
    "    print(imgs.size())\n",
    "    print(labels.size())\n",
    "    print(labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 20, 20]) torch.Size([16, 4, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False, groups=1): # default half the size\n",
    "        super(Conv, self).__init__()\n",
    "        # (w-k+2p)/s + 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Sequential(\n",
    "                    Conv(channels, channels//2, kernel_size=1, stride=1, padding=0),\n",
    "                    Conv(channels//2, channels, kernel_size=3, stride=1, padding=1)\n",
    "                ) for _ in range(num_repeats)])\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=4, num_anchors=3):\n",
    "        super(Head, self).__init__()\n",
    "        self.cv1 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1), \n",
    "                                 nn.Conv2d(in_channels // 2, num_anchors*4, kernel_size=1))\n",
    "        self.cv2 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1),\n",
    "                                 nn.Conv2d(in_channels // 2, num_classes, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_bbox = self.cv1(x)\n",
    "        pred_cls = self.cv2(x)\n",
    "        return pred_bbox, pred_cls\n",
    "\n",
    "class MyYolo(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=4, num_anchors=3):\n",
    "        super(MyYolo, self).__init__()\n",
    "\n",
    "        self.architecture = [\n",
    "            (in_channels, 32), #0 output 320\n",
    "            (32, 64), #1 160\n",
    "            (\"R\", 64),\n",
    "            (64, 128), #3 80\n",
    "            (\"R\", 128),\n",
    "            (128, 256), #5 40\n",
    "            (\"R\", 256),\n",
    "            (256, 512), #7 20\n",
    "            (\"R\", 512),\n",
    "            (\"H\", 512, num_classes, num_anchors) #9 bbox, cls\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.ModuleList([self._make_layers(param) for param in self.architecture])\n",
    "\n",
    "    def _make_layers(self, param):\n",
    "        if param[0] == \"R\":\n",
    "            layer = ResidualBlock(param[1])\n",
    "        elif param[0] == \"H\":\n",
    "            layer = Head(param[1], param[2], param[3])\n",
    "        else:\n",
    "            layer = Conv(param[0], param[1])\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = MyYolo(num_anchors=1)\n",
    "x = torch.randn(16, 3, 640, 640)\n",
    "out = model(x)\n",
    "print(out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, x, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代数据集\n",
    "batch_size = 16\n",
    "# data_loader = build_dataset(yaml_path=\"/home/szt/projects/ultralytics/urpc2020.yaml\", batch_size=batch_size, data_transform=None)\n",
    "data_loader = build_dataset(batch_size=batch_size, data_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from yolov8\n",
    "\n",
    "import numpy as np\n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): the bounding boxes to clip\n",
    "        shape (tuple): the shape of the image\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor | numpy.ndarray): Clipped boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n",
    "        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1\n",
    "        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1\n",
    "        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2\n",
    "        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "    return boxes\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    dw = x[..., 2] / 2  # half-width\n",
    "    dh = x[..., 3] / 2  # half-height\n",
    "    y[..., 0] = x[..., 0] - dw  # top left x\n",
    "    y[..., 1] = x[..., 1] - dh  # top left y\n",
    "    y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y,\n",
    "    width and height are normalized to image dimensions.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "        w (int): The width of the image. Defaults to 640\n",
    "        h (int): The height of the image. Defaults to 640\n",
    "        clip (bool): If True, the boxes will be clipped to the image boundaries. Defaults to False\n",
    "        eps (float): The minimum value of the box's width and height. Defaults to 0.0\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height, normalized) format\n",
    "    \"\"\"\n",
    "    if clip:\n",
    "        x = clip_boxes(x, (h - eps, w - eps))\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred label transform\n",
    "def get_pred_label(x, num_classes=4, anchors=torch.tensor([[10, 10]])):\n",
    "    pred_bbox, pred_cls = x\n",
    "\n",
    "    num_anchors = len(anchors)\n",
    "    batch_size, _, grid_size, _ = pred_bbox.shape\n",
    "    stride = 640 // grid_size\n",
    "\n",
    "    pred_bbox = pred_bbox.view(batch_size, num_anchors*4, pred_bbox.size(2), pred_bbox.size(3)).permute(0, 2, 3, 1).contiguous() # b, h, w, 4*a\n",
    "    x_offset, y_offset = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "    x_offset = x_offset.to(pred_bbox.device).float()\n",
    "    y_offset = y_offset.to(pred_bbox.device).float()\n",
    "    x_center = (torch.sigmoid(pred_bbox[..., 0]) + x_offset) * stride\n",
    "    y_center = (torch.sigmoid(pred_bbox[..., 1]) + y_offset) * stride\n",
    "    width = torch.sigmoid(pred_bbox[..., 2]) * anchors[:, 0].view(1, 1, 1)\n",
    "    height = torch.sigmoid(pred_bbox[..., 3]) * anchors[:, 1].view(1, 1, 1)\n",
    "    x_min = x_center - width / 2.0\n",
    "    y_min = y_center - height / 2.0\n",
    "    x_max = x_center + width / 2.0\n",
    "    y_max = y_center + height / 2.0\n",
    "    decoded_boxes = torch.stack([x_min, y_min, x_max, y_max], dim=-1)\n",
    "    decoded_boxes = decoded_boxes.view(batch_size, -1, num_anchors*4) # b, h*w, num_class, num_anchors*4\n",
    "\n",
    "    pred_cls = pred_cls.view(batch_size, num_classes, pred_cls.size(2), pred_cls.size(3)).permute(0, 2, 3, 1).contiguous() # b, h, w, c\n",
    "    pred_cls = pred_cls.view(batch_size, -1, num_classes) # b, h*w, num_class\n",
    "    pred_cls = torch.sigmoid(pred_cls)\n",
    "    return decoded_boxes, pred_cls\n",
    "\n",
    "import numpy as np\n",
    "def bbox_iou(bbox1, bbox2, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two bounding boxes.\n",
    "    :param bbox1: bounding box No.1.\n",
    "    :param bbox2: bounding box No.2.\n",
    "    :return: IoU of bbox1 and bbox2.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    xmin1, ymin1 = int(x1 - w1 / 2.0), int(y1 - h1 / 2.0)\n",
    "    xmax1, ymax1 = int(x1 + w1 / 2.0), int(y1 + h1 / 2.0)\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    xmin2, ymin2 = int(x2 - w2 / 2.0), int(y2 - h2 / 2.0)\n",
    "    xmax2, ymax2 = int(x2 + w2 / 2.0), int(y2 + h2 / 2.0)\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    xx1 = np.max([xmin1, xmin2])\n",
    "    yy1 = np.max([ymin1, ymin2])\n",
    "    xx2 = np.min([xmax1, xmax2])\n",
    "    yy2 = np.min([ymax1, ymax2])\n",
    "\n",
    "    # Calculate intersection area\n",
    "    w = np.max([0.0, xx2 - xx1 + 1])\n",
    "    h = np.max([0.0, yy2 - yy1 + 1])\n",
    "    area_intersection = w * h\n",
    "\n",
    "    # Calculate union area (subtract overlapping area to avoid double counting)\n",
    "    area1 = (xmax1 - xmin1 + 1) * (ymax1 - ymin1 + 1)\n",
    "    area2 = (xmax2 - xmin2 + 1) * (ymax2 - ymin2 + 1)\n",
    "    area_union = area1 + area2 - area_intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = area_intersection / (area_union + eps)\n",
    "    return iou\n",
    "\n",
    "def bboxes_iou(bbox1, bboxes):\n",
    "    # return iou of bbox1 and bboxes\n",
    "    ious = []\n",
    "    for bbox2 in bboxes:\n",
    "        iou = bbox_iou(bbox1, bbox2)\n",
    "        ious.append(iou)\n",
    "    return torch.tensor(ious)\n",
    "\n",
    "\n",
    "# nms, non-maximum suppression\n",
    "def nms_(pred_bbox, pred_cls, iou_threshold=0.5, cls_threshold=0.3):\n",
    "    # pred_bbox: h*w, num_anchors*4\n",
    "    # pred_cls: h*w\n",
    "    indices = []\n",
    "    \n",
    "    # according to cls scores, sort the bbox\n",
    "    sorted_indices = torch.argsort(pred_cls, descending=True)\n",
    "    while sorted_indices.size(0) > 0:\n",
    "        i = sorted_indices[0]\n",
    "        if(pred_cls[i] < cls_threshold): break\n",
    "        indices.append(i)\n",
    "        if sorted_indices.size(0) == 1:\n",
    "            break\n",
    "        iou = bboxes_iou(pred_bbox[i], pred_bbox[sorted_indices[1:]])\n",
    "        sorted_indices = sorted_indices[1:][iou < iou_threshold]\n",
    "    # \n",
    "    return torch.stack(indices)\n",
    "\n",
    "\n",
    "# loss\n",
    "box_loss = nn.MSELoss()\n",
    "cls_loss = nn.CrossEntropyLoss()\n",
    "def calculate_loss(pred_bbox, pred_cls, gt_bboxes, gt_clss):\n",
    "    b_loss = box_loss(pred_bbox, gt_bboxes)\n",
    "    c_loss = cls_loss(pred_cls, gt_clss)\n",
    "    loss = b_loss + c_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# assigner\n",
    "def pred_gt_assigner(pred_boxes, gt_boxes, iou_threshold=0.5, max_num_negatives=3):\n",
    "\n",
    "    h = len(gt_boxes)\n",
    "    w = pred_boxes.shape[0]\n",
    "    assgined_dict = {}\n",
    "    for i in range(h):\n",
    "        assgined_dict[i] = []\n",
    "\n",
    "    # 遍历每一个真实标签框  \n",
    "    for i, gt_box in enumerate(gt_boxes):  \n",
    "        \n",
    "        # 遍历每一个预测框  \n",
    "        for j, pred_box in enumerate(pred_boxes):  \n",
    "            # 计算IoU  \n",
    "            iou = bbox_iou(gt_box, pred_box)\n",
    "            if(iou > iou_threshold):\n",
    "                assgined_dict[i].append(j)\n",
    "    \n",
    "    return assgined_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:03<51:44, 63.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0, time:  63.3628  , loss: 2954.2907 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [02:06<50:41, 63.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, time:  63.3602  , loss: 2974.3583 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [03:10<49:40, 63.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    2, time:  63.4604  , loss: 2957.4438 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [04:13<48:36, 63.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    3, time:  63.4069  , loss: 2950.3857 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [05:17<47:34, 63.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    4, time:  63.4838  , loss: 2983.6090 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [06:20<46:30, 63.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    5, time:  63.4039  , loss: 2978.7689 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [07:23<45:26, 63.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    6, time:  63.3402  , loss: 2966.8249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [08:27<44:23, 63.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    7, time:  63.4326  , loss: 2954.3005 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [09:30<43:17, 63.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    8, time:  63.2317  , loss: 2970.1861 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [10:33<42:15, 63.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    9, time:  63.4564  , loss: 2973.4811 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [10:53<43:33, 65.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m<\u001b[39m b_t):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# first! consider output box as center and width, height\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pred_bboxes_xyxy, pred_clss \u001b[38;5;241m=\u001b[39m get_pred_label(preds)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# when training , do not 执行NMS\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# keep_indices = [[nms(bbox, pred_clss[i].max(dim=1)[0], 0.5)] for i in range(pred_clss.shape[0])] # spend batch_size * 6.5s, need to optimize\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# keep_indices = [nms(pred_bboxes_xyxy[i], pred_clss[i].max(dim=1)[0], 0.5) for i in range(b_t)]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# keep_indices中存储了保留的预测框的索引\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 可以根据这些索引获取最终的目标框\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mMyYolo.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_residual:\n\u001b[0;32m---> 29\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.ops import nms\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, weight_decay=0.005)  # 使用随机梯度下降优化器  \n",
    "epochs = 50  # 训练100轮\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss_t = []\n",
    "    for batch in data_loader:\n",
    "        features, labels = batch\n",
    "        b_t = features.shape[0]\n",
    "        labels = group_adjust(labels, b_t) # b, n, 5; list of tensor\n",
    "        if(len(labels) < b_t):\n",
    "            continue\n",
    "        \n",
    "        preds = model(features) # first! consider output box as center and width, height\n",
    "        pred_bboxes_xyxy, pred_clss = get_pred_label(preds)\n",
    "        \n",
    "        # when training , do not 执行NMS\n",
    "        # keep_indices = [[nms(bbox, pred_clss[i].max(dim=1)[0], 0.5)] for i in range(pred_clss.shape[0])] # spend batch_size * 6.5s, need to optimize\n",
    "        # keep_indices = [nms(pred_bboxes_xyxy[i], pred_clss[i].max(dim=1)[0], 0.5) for i in range(b_t)]\n",
    "\n",
    "        # keep_indices中存储了保留的预测框的索引\n",
    "        # 可以根据这些索引获取最终的目标框\n",
    "\n",
    "        gt_clss = [labels[i][:, 0] for i in range(b_t)]# n\n",
    "        gt_clss = [gt_clss[i].type(torch.long) for i in range(b_t)]\n",
    "        gt_bboxes = [labels[i][:, 1:] for i in range(b_t)]# n, 4\n",
    "        gt_bboxes_xyxy = [xywh2xyxy(gt_bboxes[i]) * 640 for i in range(b_t)]\n",
    "\n",
    "        # task assigner\n",
    "        loss = torch.tensor(0)\n",
    "        n = 0\n",
    "        assigned_dicts = [pred_gt_assigner(gt_boxes=gt_bboxes_xyxy[i], pred_boxes=pred_bboxes_xyxy[i], iou_threshold=0.5, max_num_negatives=1) for i in range(b_t)]\n",
    "        for i in range(b_t):  # 针对没张图片\n",
    "            gt_bboxes_xyxy_i = gt_bboxes_xyxy[i]\n",
    "            gt_clss_i = gt_clss[i]\n",
    "            assigned_dict = assigned_dicts[i]\n",
    "\n",
    "            for b_i in range(len(gt_bboxes_xyxy_i)): # 针对图中的的每一个目标框\n",
    "                if b_i in assigned_dict:\n",
    "                    pred_bbox = pred_bboxes_xyxy[i][assigned_dict[b_i]]\n",
    "                    pred_cls = pred_clss[i][assigned_dict[b_i]]\n",
    "                    gt_bbox = gt_bboxes_xyxy_i[b_i]\n",
    "                    gt_cls = gt_clss_i[b_i]\n",
    "                    for k in range(len(pred_bbox)):\n",
    "                        loss = loss + calculate_loss(pred_bbox[k], pred_cls[k], gt_bbox, gt_cls)\n",
    "                        n = n + 1\n",
    "        loss = loss / n\n",
    "        loss_t.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"epoch: {epoch:4}, time: {time:^10.4f}, loss: {loss:^10.4f}\".format(epoch=epoch, time=end_time-start_time, loss=np.mean(loss_t)))\n",
    "\n",
    "# bbox1 = torch.tensor([100, 100, 50, 50])\n",
    "# bbox2 = torch.tensor([75, 75, 50, 50])\n",
    "# iou = bbox_iou(bbox1, bbox2)\n",
    "# print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"hg_model.pth\") # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = Image.open(\"/home/szt/datasets/hg_underwater_target/hard_samples_test/1702521478.434.3239.jpeg\").convert(\"RGB\")\n",
    "original_img = cv2.imread(\"/home/szt/datasets/hg_underwater_target/hard_samples_test/1702521478.434.3239.jpeg\", cv2.COLOR_BGR2RGB)\n",
    "original_img = cv2.resize(original_img, (640, 640))\n",
    "data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "img = data_transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "out = model(img)\n",
    "print(len(out))\n",
    "print(out[0].shape, out[1].shape) # bbox, cls\n",
    "\n",
    "bboxes, clss = get_pred_label(out)\n",
    "bt = bboxes.shape[0]\n",
    "print(bboxes[0].shape, clss[0].shape)\n",
    "\n",
    "# 执行NMS\n",
    "keep_indices = [nms_(bboxes[i], clss[i].max(dim=1)[0], 0.3, 0.7) for i in range(bt)]\n",
    "print(keep_indices)\n",
    "print(bboxes[0].shape, clss[0].shape)\n",
    "# keep_indices中存储了保留的预测框的索引\n",
    "# 可以根据这些索引获取最终的目标框\n",
    "bbox = [bboxes[i][keep_indices[i]] for i in range(0, bt)]\n",
    "score = [clss[i][keep_indices[i]].max(dim=1)[0] for i in range(0, bt)]\n",
    "print(bbox[0].shape, score[0].shape)\n",
    "\n",
    "# filer score\n",
    "for box, s in zip(bbox[0], score[0]):\n",
    "    cv2.rectangle(original_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 1)\n",
    "cv2.imwrite(\"test.jpg\", original_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
