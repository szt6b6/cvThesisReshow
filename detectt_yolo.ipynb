{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from yaml import safe_load\n",
    "from os.path import join, exists, getsize\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "def collate_fn(data):\n",
    "    imgs, labels = zip(*data)\n",
    "    assert len(imgs) == len(labels)\n",
    " \n",
    "    for i, lb in enumerate(labels):\n",
    "        lb[:, 0] = i  # add target image index for build_targets()\n",
    "\n",
    "    labels = torch.cat(labels, 0)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "class URPC2020_DataSet(Dataset):\n",
    "    def __init__(self, data_yaml_path=\"hg.yaml\", type=\"val\", transforms=None):\n",
    "        self.data_yaml_path = data_yaml_path\n",
    "        self.transforms = transforms\n",
    "        with open(self.data_yaml_path, 'r') as file:\n",
    "            self.data_dict = safe_load(file)\n",
    "        self.nc = self.data_dict['nc']\n",
    "        self.root_path = self.data_dict[\"path\"]\n",
    "        self.classes = self.data_dict[\"names\"]\n",
    "\n",
    "        if(type == \"train\"):\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"train\"])\n",
    "        elif type == \"val\":\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"val\"])\n",
    "        else:\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"test\"])\n",
    "\n",
    "        self.labels_path = f\"{self.root_path}/{type}/labels\"\n",
    "        labels = listdir(self.labels_path)\n",
    "        self.labels = self.filter_labels(labels)\n",
    "    \n",
    "    def filter_labels(self, labels):\n",
    "        # filter empty labels\n",
    "        labels = [label for label in labels if getsize(join(self.labels_path, label)) != 0]\n",
    "        return labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = label[:-3] + \"jpeg\" #jpg\n",
    "        # image = label[:-3] + \"jpg\"\n",
    "        image_path = f\"{self.images_path}/{image}\"\n",
    "        label_path = f\"{self.labels_path}/{label}\"\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        label = []\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                line_strip = line.strip().split(\" \") #index in batch, cls, bbox\n",
    "                label.append([int(0), int(line_strip[0]), float(line_strip[1]), float(line_strip[2]), float(line_strip[3]), float(line_strip[4])])\n",
    "        img = self.transforms(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def group_adjust(labels, batch_size):\n",
    "    # 创建一个字典，按照第一列的值进行分组\n",
    "    grouped_labels = [[] for i in range(batch_size)]\n",
    "    for row in iter(labels):\n",
    "        i = int(row[0].item())\n",
    "        grouped_labels[i].append(row[1:])\n",
    "    try:\n",
    "        grouped_labels = [torch.stack(group) for group in grouped_labels]\n",
    "    finally:\n",
    "        return grouped_labels\n",
    "\n",
    "\n",
    "def build_dataset(yaml_path=\"hg.yaml\", batch_size=16, data_transform=None):\n",
    "    if(data_transform is None):\n",
    "        data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            ])\n",
    "    # Create a custom dataset\n",
    "    dataset = URPC2020_DataSet(yaml_path, transforms=data_transform)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=4,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([18, 6])\n",
      "tensor([0.0000, 1.0000, 0.8356, 0.4021, 0.0234, 0.0309])\n",
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([18, 6])\n",
      "tensor([0.0000, 1.0000, 0.2440, 0.4335, 0.0316, 0.0382])\n",
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([16, 6])\n",
      "tensor([0.0000, 1.0000, 0.7019, 0.5897, 0.0340, 0.0421])\n",
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([16, 6])\n",
      "tensor([0.0000, 1.0000, 0.4841, 0.8573, 0.0328, 0.0509])\n"
     ]
    }
   ],
   "source": [
    "# data_loader = build_dataset(yaml_path=\"/home/szt/projects/ultralytics/urpc2020.yaml\")\n",
    "data_loader = build_dataset()\n",
    "for i, (imgs, labels) in enumerate(data_loader):\n",
    "    # print(imgs[0] * 255)\n",
    "    print(imgs.size())\n",
    "    print(labels.size())\n",
    "    print(labels[0])\n",
    "    if i > 2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 20, 20]) torch.Size([16, 4, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False, groups=1): # default half the size\n",
    "        super(Conv, self).__init__()\n",
    "        # (w-k+2p)/s + 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Sequential(\n",
    "                    Conv(channels, channels//2, kernel_size=1, stride=1, padding=0),\n",
    "                    Conv(channels//2, channels, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.LeakyReLU()\n",
    "                ) for _ in range(num_repeats)])\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=4, num_anchors=3):\n",
    "        super(Head, self).__init__()\n",
    "        self.cv1 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1), \n",
    "                                 nn.Conv2d(in_channels // 2, num_anchors*5, kernel_size=1), nn.Sigmoid())\n",
    "        self.cv2 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1),\n",
    "                                 nn.Conv2d(in_channels // 2, num_classes, kernel_size=1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_bbox = self.cv1(x)\n",
    "        pred_cls = self.cv2(x)\n",
    "        return pred_bbox, pred_cls\n",
    "\n",
    "class MyYolo(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=4, num_anchors=3):\n",
    "        super(MyYolo, self).__init__()\n",
    "\n",
    "        self.architecture = [\n",
    "            (in_channels, 64), #0 output 320\n",
    "            (64, 128), #1 160\n",
    "            # (\"R\", 64),\n",
    "            (128, 256), #3 80\n",
    "            # (\"R\", 128),\n",
    "            (256, 512), #5 40\n",
    "            # (\"R\", 256),\n",
    "            (512, 256), #7 20\n",
    "            # (\"R\", 512),\n",
    "            (\"H\", 256, num_classes, num_anchors) #9 bbox, cls\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.ModuleList([self._make_layers(param) for param in self.architecture])\n",
    "\n",
    "    def _make_layers(self, param):\n",
    "        if param[0] == \"R\":\n",
    "            layer = ResidualBlock(param[1])\n",
    "        elif param[0] == \"H\":\n",
    "            layer = Head(param[1], param[2], param[3])\n",
    "        else:\n",
    "            layer = Conv(param[0], param[1])\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = MyYolo(num_anchors=1)\n",
    "x = torch.randn(16, 3, 640, 640)\n",
    "out = model(x)\n",
    "print(out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6075, 0.4703, 0.6192,  ..., 0.7241, 0.6771, 0.6529],\n",
      "         [0.6089, 0.4849, 0.4578,  ..., 0.3859, 0.5285, 0.6075],\n",
      "         [0.5780, 0.4753, 0.4189,  ..., 0.6317, 0.5106, 0.5558],\n",
      "         ...,\n",
      "         [0.6844, 0.4472, 0.5856,  ..., 0.5703, 0.5543, 0.4631],\n",
      "         [0.6090, 0.5100, 0.4512,  ..., 0.5842, 0.6372, 0.5784],\n",
      "         [0.6323, 0.5441, 0.4877,  ..., 0.3968, 0.5896, 0.6099]],\n",
      "\n",
      "        [[0.5469, 0.4532, 0.6111,  ..., 0.5280, 0.4268, 0.4072],\n",
      "         [0.5500, 0.5316, 0.5149,  ..., 0.5007, 0.5976, 0.4753],\n",
      "         [0.5883, 0.4758, 0.4316,  ..., 0.5264, 0.7196, 0.5177],\n",
      "         ...,\n",
      "         [0.5940, 0.5914, 0.6142,  ..., 0.6765, 0.6312, 0.5553],\n",
      "         [0.5340, 0.6161, 0.5746,  ..., 0.4790, 0.4905, 0.3956],\n",
      "         [0.5785, 0.5229, 0.5069,  ..., 0.5929, 0.6472, 0.4688]],\n",
      "\n",
      "        [[0.5237, 0.5032, 0.4582,  ..., 0.4228, 0.5642, 0.5198],\n",
      "         [0.5171, 0.4279, 0.4945,  ..., 0.6745, 0.4865, 0.4765],\n",
      "         [0.5206, 0.5412, 0.5162,  ..., 0.3948, 0.4515, 0.5265],\n",
      "         ...,\n",
      "         [0.4575, 0.4715, 0.6620,  ..., 0.5216, 0.4031, 0.4975],\n",
      "         [0.5279, 0.5240, 0.4374,  ..., 0.3858, 0.5875, 0.5103],\n",
      "         [0.4626, 0.5164, 0.7063,  ..., 0.6383, 0.5621, 0.5049]],\n",
      "\n",
      "        [[0.4692, 0.3960, 0.4714,  ..., 0.3930, 0.4363, 0.3894],\n",
      "         [0.4045, 0.3852, 0.4245,  ..., 0.3942, 0.3180, 0.5161],\n",
      "         [0.5585, 0.2724, 0.3723,  ..., 0.5820, 0.4341, 0.3604],\n",
      "         ...,\n",
      "         [0.5705, 0.3859, 0.4117,  ..., 0.5080, 0.3463, 0.4329],\n",
      "         [0.3863, 0.3514, 0.4506,  ..., 0.4497, 0.3924, 0.4441],\n",
      "         [0.4224, 0.3666, 0.3955,  ..., 0.3479, 0.3972, 0.3593]],\n",
      "\n",
      "        [[0.4401, 0.4570, 0.4184,  ..., 0.4442, 0.4631, 0.3958],\n",
      "         [0.5290, 0.4728, 0.4453,  ..., 0.4084, 0.5031, 0.3895],\n",
      "         [0.4430, 0.6477, 0.5038,  ..., 0.4030, 0.4415, 0.4614],\n",
      "         ...,\n",
      "         [0.4434, 0.4718, 0.5414,  ..., 0.4613, 0.3189, 0.3352],\n",
      "         [0.3425, 0.3878, 0.4801,  ..., 0.4168, 0.4981, 0.5455],\n",
      "         [0.4244, 0.4078, 0.3189,  ..., 0.3721, 0.2530, 0.3946]]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, x, \"model.onnx\")\n",
    "print(out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代数据集\n",
    "batch_size = 16\n",
    "# data_loader = build_dataset(yaml_path=\"/home/szt/projects/ultralytics/urpc2020.yaml\", batch_size=batch_size, data_transform=None)\n",
    "data_loader = build_dataset(batch_size=batch_size, data_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from yolov8\n",
    "\n",
    "import numpy as np\n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): the bounding boxes to clip\n",
    "        shape (tuple): the shape of the image\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor | numpy.ndarray): Clipped boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n",
    "        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1\n",
    "        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1\n",
    "        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2\n",
    "        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "    return boxes\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    dw = x[..., 2] / 2  # half-width\n",
    "    dh = x[..., 3] / 2  # half-height\n",
    "    y[..., 0] = x[..., 0] - dw  # top left x\n",
    "    y[..., 1] = x[..., 1] - dh  # top left y\n",
    "    y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y,\n",
    "    width and height are normalized to image dimensions.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "        w (int): The width of the image. Defaults to 640\n",
    "        h (int): The height of the image. Defaults to 640\n",
    "        clip (bool): If True, the boxes will be clipped to the image boundaries. Defaults to False\n",
    "        eps (float): The minimum value of the box's width and height. Defaults to 0.0\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height, normalized) format\n",
    "    \"\"\"\n",
    "    if clip:\n",
    "        x = clip_boxes(x, (h - eps, w - eps))\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bbox_iou(bbox1, bbox2, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two bounding boxes.\n",
    "    :param bbox1: bounding box No.1.\n",
    "    :param bbox2: bounding box No.2.\n",
    "    :return: IoU of bbox1 and bbox2.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1, _ = bbox1\n",
    "    xmin1, ymin1 = x1 - w1 / 2.0, y1 - h1 / 2.0\n",
    "    xmax1, ymax1 = x1 + w1 / 2.0, y1 + h1 / 2.0\n",
    "    x2, y2, w2, h2, _ = bbox2\n",
    "    xmin2, ymin2 = x2 - w2 / 2.0, y2 - h2 / 2.0\n",
    "    xmax2, ymax2 = x2 + w2 / 2.0, y2 + h2 / 2.0\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    xx1 = np.max([xmin1, xmin2])\n",
    "    yy1 = np.max([ymin1, ymin2])\n",
    "    xx2 = np.min([xmax1, xmax2])\n",
    "    yy2 = np.min([ymax1, ymax2])\n",
    "\n",
    "    # Calculate intersection area\n",
    "    w = np.max([0.0, xx2 - xx1])\n",
    "    h = np.max([0.0, yy2 - yy1])\n",
    "    area_intersection = w * h\n",
    "\n",
    "    # Calculate union area (subtract overlapping area to avoid double counting)\n",
    "    area1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    area2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "    area_union = area1 + area2 - area_intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = area_intersection / (area_union + eps)\n",
    "    return iou\n",
    "\n",
    "def bboxes_iou(bbox1, bboxes):\n",
    "    # return iou of bbox1 and bboxes\n",
    "    ious = []\n",
    "    for bbox2 in bboxes:\n",
    "        iou = bbox_iou(bbox1.detach().numpy(), bbox2.detach().numpy())\n",
    "        ious.append(iou)\n",
    "    return torch.tensor(ious)\n",
    "\n",
    "\n",
    "\n",
    "# loss\n",
    "box_loss = nn.MSELoss()\n",
    "cls_loss = nn.CrossEntropyLoss()\n",
    "def calculate_loss(pred_bbox, pred_cls, gt_bboxes, gt_clss):\n",
    "    b_loss = box_loss(pred_bbox, gt_bboxes)\n",
    "    c_loss = cls_loss(pred_cls, gt_clss)\n",
    "    loss = b_loss + c_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# assigner\n",
    "def pred_gt_assigner(pred_boxes, gt_boxes, iou_threshold=0.5, max_num_negatives=3):\n",
    "\n",
    "    h = len(gt_boxes)\n",
    "    w = pred_boxes.shape[0]\n",
    "    assgined_dict = {}\n",
    "    for i in range(h):\n",
    "        assgined_dict[i] = []\n",
    "\n",
    "    # 遍历每一个真实标签框  \n",
    "    for i, gt_box in enumerate(gt_boxes):  \n",
    "        \n",
    "        # 遍历每一个预测框  \n",
    "        for j, pred_box in enumerate(pred_boxes):  \n",
    "            # 计算IoU  \n",
    "            iou = bbox_iou(gt_box, pred_box)\n",
    "            if(iou > iou_threshold):\n",
    "                assgined_dict[i].append(j)\n",
    "    \n",
    "    return assgined_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [08:09<1:13:23, 489.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0, time:  489.2695 , loss:   0.0893  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [16:16<1:05:06, 488.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, time:  487.6144 , loss:   0.0270  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [24:24<56:55, 487.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    2, time:  487.5502 , loss:   0.0182  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [32:28<48:37, 486.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    3, time:  483.6625 , loss:   0.0148  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [40:29<40:22, 484.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    4, time:  481.3634 , loss:   0.0129  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [48:32<32:15, 483.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    5, time:  482.5470 , loss:   0.0120  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [56:34<24:09, 483.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    6, time:  482.1927 , loss:   0.0112  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:04:36<16:05, 482.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    7, time:  482.1649 , loss:   0.0106  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:12:38<08:02, 482.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    8, time:  482.3536 , loss:   0.0101  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:20:40<00:00, 484.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    9, time:  481.6631 , loss:   0.0098  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.ops import nms\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, weight_decay=0.005)  # 使用随机梯度下降优化器  \n",
    "epochs = 10  # 训练10轮\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss_t = []\n",
    "    for e, batch in enumerate(data_loader):\n",
    "        features, labels = batch\n",
    "        b_t = features.shape[0]\n",
    "        labels = group_adjust(labels, b_t) # b, n, 5; list of tensor\n",
    "        if(len(labels) < b_t):\n",
    "            continue\n",
    "        \n",
    "        # b, 5, 20, 20; b, 4, 20, 20\n",
    "        pred_boxes, pred_classes = model(features) # first! consider output box as center and width, height\n",
    "        pred_boxes = pred_boxes.permute(0, 2, 3, 1)\n",
    "        pred_classes = pred_classes.permute(0, 2, 3, 1)\n",
    "\n",
    "        # b, 20, 20, 5; b, 20, 20, 4\n",
    "        gt_boxes = torch.zeros(b_t, 20, 20, 5)\n",
    "        gt_classes = torch.zeros(b_t, 20, 20, 4)\n",
    "        for i, label in enumerate(labels):\n",
    "            for j, lable_ in enumerate(label):\n",
    "                x_idx, y_idx = int(lable_[1] * 20), int(lable_[2] * 20)\n",
    "                x_idx = np.clip(x_idx, 0, 19)\n",
    "                y_idx = np.clip(y_idx, 0, 19)\n",
    "                gt_boxes[i, x_idx, y_idx, 0] = lable_[1] * 20.0 - x_idx\n",
    "                gt_boxes[i, x_idx, y_idx, 1] = lable_[2] * 20.0 - y_idx\n",
    "                gt_boxes[i, x_idx, y_idx, 2:4] = lable_[3:]\n",
    "                gt_boxes[i, x_idx, y_idx, 4] = 1\n",
    "                gt_classes[i, x_idx, y_idx, int(lable_[0])] = 1\n",
    "        \n",
    "        have_obg = (gt_boxes[..., 4] == 1)\n",
    "        no_obj = ~have_obg\n",
    "\n",
    "        # print(gt_boxes.shape, gt_classes.shape)\n",
    "        # print(have_obg.shape)\n",
    "\n",
    "        # print(pred_boxes[0], gt_boxes[0])\n",
    "        loss_coor = ((gt_boxes[..., :2] - pred_boxes[..., :2]) ** 2 \\\n",
    "                    + (torch.sqrt(pred_boxes[..., 2:4]) - torch.sqrt(gt_boxes[..., 2:4])) ** 2).sum(dim=-1) * have_obg\n",
    "        \n",
    "        loss_confidence = (gt_boxes[..., 4] - pred_boxes[..., 4]) ** 2\n",
    "        # print(loss_coor.shape)\n",
    "\n",
    "        loss_class = ((pred_classes - gt_classes) ** 2).sum(dim=-1) * have_obg\n",
    "        # print(loss_class.shape)\n",
    "        \n",
    "        loss_noOb = (gt_boxes[..., 4] - pred_boxes[..., 4]) ** 2 * no_obj\n",
    "\n",
    "        loss = (1.0 * loss_coor + loss_confidence + loss_class + 0.1 * loss_noOb).mean()\n",
    "\n",
    "        loss_t.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"epoch: {epoch:4}, time: {time:^10.4f}, loss: {loss:^10.4f}\".format(epoch=epoch, time=end_time-start_time, loss=np.mean(loss_t)))\n",
    "\n",
    "# bbox1 = torch.tensor([100, 100, 50, 50])\n",
    "# bbox2 = torch.tensor([75, 75, 50, 50])\n",
    "# iou = bbox_iou(bbox1, bbox2)\n",
    "# print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"hg_model.pth\") # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 20]) torch.Size([4, 20, 20])\n",
      "torch.Size([400, 1]) torch.Size([400, 5])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = Image.open(\"/home/szt/datasets/hg_underwater_target/纯色曝光1000us距离1m正常水下不同角度位置橙色背景/1702521983.607.24.jpeg\").convert(\"RGB\")\n",
    "original_img = cv2.imread(\"/home/szt/datasets/hg_underwater_target/纯色曝光1000us距离1m正常水下不同角度位置橙色背景/1702521983.607.24.jpeg\", cv2.COLOR_BGR2RGB)\n",
    "original_img = cv2.resize(original_img, (640, 640))\n",
    "data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            ])\n",
    "model = torch.load(\"./hg_model.pth\")\n",
    "img = data_transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "out = model(img)\n",
    "bboxes, clss = out\n",
    "\n",
    "bt = bboxes.shape[0]\n",
    "print(bboxes[0].shape, clss[0].shape)\n",
    "box = bboxes[0].permute(1,2,0)\n",
    "cs = clss[0].permute(1,2,0)\n",
    "\n",
    "# 生成预测框\n",
    "pred_boxes = torch.zeros_like(box)\n",
    "pred_score = torch.zeros(20, 20, 1)\n",
    "pred_class = torch.zeros(20, 20, 1)\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        x, y, w, h, confidence = (box[i][j][0] + i) * 640, (box[i][j][1] + j) / 20.0 * 640, box[i][j][2] * 640, box[i][j][3] * 640, box[i][j][4]\n",
    "        pred_boxes[i, j, :] = torch.stack([x, y, w, h, confidence])\n",
    "        score, cata = cs[i][j].max(dim=-1)\n",
    "        pred_class[i, j, :] = cata\n",
    "        pred_score[i, j, :] = score\n",
    "\n",
    "pred_boxes = pred_boxes.view(-1, 5)\n",
    "pred_class = pred_class.view(-1, 1)\n",
    "pred_score = pred_score.view(-1, 1)\n",
    "print(pred_class.shape, pred_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nms, non-maximum suppression\n",
    "def nms_(pred_bbox, iou_threshold=0.5, score_threshold=0.3):\n",
    "    # pred_bbox: h*w, num_anchors*4\n",
    "    # pred_cls: h*w\n",
    "    indices = []\n",
    "    \n",
    "    # according to cls scores, sort the bbox\n",
    "    confidence = pred_bbox[..., 4]\n",
    "    # print(confidence.shape)\n",
    "    sorted_indices = torch.argsort(confidence, descending=True)\n",
    "    # print(sorted_indices)\n",
    "    while sorted_indices.size(0) > 0:\n",
    "        i = sorted_indices[0]\n",
    "        if(confidence[i] < score_threshold): break\n",
    "        indices.append(i)\n",
    "        if sorted_indices.size(0) == 1:\n",
    "            break\n",
    "        iou = bboxes_iou(pred_bbox[i], pred_bbox[sorted_indices[1:]])\n",
    "        sorted_indices = sorted_indices[1:][iou < iou_threshold]\n",
    "    # \n",
    "    return  [] if len(indices) == 0 else torch.stack(indices) \n",
    "\n",
    "# 执行NMS\n",
    "keep_indices = nms_(pred_boxes, 0.5, 0.05)\n",
    "# print(keep_indices)\n",
    "# print(pred_boxes[keep_indices], pred_class[keep_indices], pred_score[keep_indices])\n",
    "\n",
    "# keep_indices中存储了保留的预测框的索引\n",
    "# 可以根据这些索引获取最终的目标框\n",
    "# filer score\n",
    "for box, s in zip(pred_boxes[keep_indices], pred_class[keep_indices]):\n",
    "    cv2.putText(original_img, str(int(s.item())), (int(box[0] - box[2] / 2), int(box[1] - box[3] / 2)), cv2.FONT_ITALIC, 1.0, (0, 255, 0), 1)\n",
    "    cv2.rectangle(original_img, (int(box[0] - box[2] / 2), int(box[1] - box[3] / 2)), (int(box[0] + box[2] / 2), int(box[1] + box[3] / 2)), (0, 255, 0), 1)\n",
    "cv2.imwrite(\"test.jpg\", original_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "original_img = cv2.imread(\"/home/szt/datasets/hg_underwater_target/hard_samples_test/1702521478.434.3239.jpeg\", cv2.COLOR_BGR2RGB)\n",
    "original_img = cv2.resize(original_img, (640, 640))\n",
    "\n",
    "for i, (imgs, labels) in enumerate(data_loader):\n",
    "    b_t = features.shape[0]\n",
    "    labels = group_adjust(labels, b_t) # b, n, 5; list of tensor\n",
    "    if(len(labels) < b_t):\n",
    "        continue\n",
    "    \n",
    "    # b, 5, 20, 20; b, 4, 20, 20\n",
    "    # pred_boxes, pred_classes = model(features) # first! consider output box as center and width, height\n",
    "    img = imgs[0].permute(1,2,0)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    print(img.shape)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for j, lable_ in enumerate(labels[0]):\n",
    "        (c, x, y, w, h) = lable_\n",
    "        minx, miny = x - w / 2, y - h / 2\n",
    "        maxx, maxy = x + w / 2, y + h / 2\n",
    "        \n",
    "        minx, miny = minx.item() * 640, miny.item() * 640\n",
    "        maxx, maxy = maxx.item() * 640, maxy.item() * 640\n",
    "        cv2.rectangle(img, (int(minx), int(miny)), (int(maxx), int(maxy)), (255, 255, 0), 1)\n",
    "\n",
    "    cv2.imwrite(\"test.jpg\", img)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
