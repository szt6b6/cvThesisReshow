{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from yaml import safe_load\n",
    "from os.path import join, exists, getsize\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "def collate_fn(data):\n",
    "    imgs, labels = zip(*data)\n",
    "    assert len(imgs) == len(labels)\n",
    " \n",
    "    for i, lb in enumerate(labels):\n",
    "        lb[:, 0] = i  # add target image index for build_targets()\n",
    "\n",
    "    labels = torch.cat(labels, 0)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "class URPC2020_DataSet(Dataset):\n",
    "    def __init__(self, data_yaml_path=\"/home/szt/projects/ultralytics/hg.yaml\", type=\"train\", transforms=None):\n",
    "        self.data_yaml_path = data_yaml_path\n",
    "        self.transforms = transforms\n",
    "        with open(self.data_yaml_path, 'r') as file:\n",
    "            self.data_dict = safe_load(file)\n",
    "        self.nc = self.data_dict['nc']\n",
    "        self.root_path = self.data_dict[\"path\"]\n",
    "        self.classes = self.data_dict[\"names\"]\n",
    "\n",
    "        if(type == \"train\"):\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"train\"])\n",
    "        elif type == \"val\":\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"val\"])\n",
    "        else:\n",
    "            self.images_path = join(self.root_path, self.data_dict[\"test\"])\n",
    "\n",
    "        self.labels_path = f\"{self.root_path}/{type}/labels\"\n",
    "        labels = listdir(self.labels_path)\n",
    "        self.labels = self.filter_labels(labels)\n",
    "    \n",
    "    def filter_labels(self, labels):\n",
    "        # filter empty labels\n",
    "        labels = [label for label in labels if getsize(join(self.labels_path, label)) != 0]\n",
    "        return labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = label[:-3] + \"jpeg\" #.jpg\n",
    "        image_path = f\"{self.images_path}/{image}\"\n",
    "        label_path = f\"{self.labels_path}/{label}\"\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        label = []\n",
    "        with open(label_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                line_strip = line.strip().split(\" \") #index in batch, cls, bbox\n",
    "                label.append([int(0), int(line_strip[0]), float(line_strip[1]), float(line_strip[2]), float(line_strip[3]), float(line_strip[4])])\n",
    "        img = self.transforms(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def group_adjust(labels, batch_size):\n",
    "    # 创建一个字典，按照第一列的值进行分组\n",
    "    grouped_labels = [[] for i in range(batch_size)]\n",
    "    for row in iter(labels):\n",
    "        i = int(row[0].item())\n",
    "        grouped_labels[i].append(row[1:])\n",
    "    try:\n",
    "        grouped_labels = [torch.stack(group) for group in grouped_labels]\n",
    "    finally:\n",
    "        return grouped_labels\n",
    "\n",
    "\n",
    "def build_dataset(yaml_path=\"/home/szt/projects/ultralytics/hg.yaml\", batch_size=16, data_transform=None):\n",
    "    if(data_transform is None):\n",
    "        data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            ])\n",
    "    # Create a custom dataset\n",
    "    dataset = URPC2020_DataSet(yaml_path, transforms=data_transform)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=4,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[15.8182, 16.0568, 14.6477,  ..., 18.5000, 17.7500, 17.9091],\n",
      "         [ 4.3228,  5.8426,  5.6065,  ...,  5.8472,  5.8889,  4.6032],\n",
      "         [ 3.3155,  3.8906,  2.6979,  ...,  5.2031,  4.4167,  3.5357],\n",
      "         ...,\n",
      "         [ 7.1786,  6.9115,  6.0625,  ...,  5.8229,  7.4688,  8.4286],\n",
      "         [ 8.2751,  8.2778,  7.0648,  ...,  3.4074,  4.9120,  7.4074],\n",
      "         [ 8.2273,  8.1477,  5.0057,  ...,  4.3466,  4.7159,  6.3052]],\n",
      "\n",
      "        [[15.8182, 16.0568, 14.3977,  ..., 18.5000, 17.7500, 17.9091],\n",
      "         [ 5.5926,  6.0278,  5.4491,  ...,  5.8472,  5.8889,  4.6032],\n",
      "         [ 5.1726,  4.8906,  2.6979,  ...,  5.2031,  4.4167,  3.5357],\n",
      "         ...,\n",
      "         [ 5.1786,  4.9115,  4.0625,  ...,  3.8229,  5.4688,  6.4286],\n",
      "         [ 6.2751,  6.2778,  5.0648,  ...,  1.4074,  2.9120,  5.4074],\n",
      "         [ 6.2273,  6.1477,  3.0057,  ...,  2.3466,  2.7159,  4.3052]],\n",
      "\n",
      "        [[13.8182, 14.3068, 14.2727,  ..., 20.2500, 19.7500, 19.9091],\n",
      "         [ 2.9577,  4.0000,  3.9815,  ...,  7.5972,  7.8889,  6.6032],\n",
      "         [ 1.3155,  1.5208,  0.8125,  ...,  6.0781,  5.4167,  4.5357],\n",
      "         ...,\n",
      "         [ 6.1786,  5.9115,  5.0625,  ...,  6.6979,  7.5938,  8.4286],\n",
      "         [ 7.2751,  7.2778,  6.0648,  ...,  4.8611,  6.4306,  8.9259],\n",
      "         [ 7.2273,  7.1477,  4.0057,  ...,  7.0966,  7.7159,  9.3052]]])\n",
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([16, 6])\n",
      "tensor([0.0000, 1.0000, 0.2216, 0.3807, 0.0382, 0.0459])\n"
     ]
    }
   ],
   "source": [
    "data_loader = build_dataset()\n",
    "for i, (imgs, labels) in enumerate(data_loader):\n",
    "    print(imgs[0] * 255)\n",
    "    print(imgs.size())\n",
    "    print(labels.size())\n",
    "    print(labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 20, 20]) torch.Size([16, 4, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False, groups=1): # default half the size\n",
    "        super(Conv, self).__init__()\n",
    "        # (w-k+2p)/s + 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Sequential(\n",
    "                    Conv(channels, channels//2, kernel_size=1, stride=1, padding=0),\n",
    "                    Conv(channels//2, channels, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.LeakyReLU()\n",
    "                ) for _ in range(num_repeats)])\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=4, num_anchors=3):\n",
    "        super(Head, self).__init__()\n",
    "        self.cv1 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1), \n",
    "                                 nn.Conv2d(in_channels // 2, num_anchors*5, kernel_size=1), nn.Sigmoid())\n",
    "        self.cv2 = nn.Sequential(Conv(in_channels, in_channels // 2, stride=1),\n",
    "                                 nn.Conv2d(in_channels // 2, num_classes, kernel_size=1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_bbox = self.cv1(x)\n",
    "        pred_cls = self.cv2(x)\n",
    "        return pred_bbox, pred_cls\n",
    "\n",
    "class MyYolo(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=4, num_anchors=3):\n",
    "        super(MyYolo, self).__init__()\n",
    "\n",
    "        self.architecture = [\n",
    "            (in_channels, 64), #0 output 320\n",
    "            (64, 128), #1 160\n",
    "            # (\"R\", 64),\n",
    "            (128, 256), #3 80\n",
    "            # (\"R\", 128),\n",
    "            (256, 512), #5 40\n",
    "            # (\"R\", 256),\n",
    "            (512, 256), #7 20\n",
    "            # (\"R\", 512),\n",
    "            (\"H\", 256, num_classes, num_anchors) #9 bbox, cls\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.ModuleList([self._make_layers(param) for param in self.architecture])\n",
    "\n",
    "    def _make_layers(self, param):\n",
    "        if param[0] == \"R\":\n",
    "            layer = ResidualBlock(param[1])\n",
    "        elif param[0] == \"H\":\n",
    "            layer = Head(param[1], param[2], param[3])\n",
    "        else:\n",
    "            layer = Conv(param[0], param[1])\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = MyYolo(num_anchors=1)\n",
    "x = torch.randn(16, 3, 640, 640)\n",
    "out = model(x)\n",
    "print(out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4888, 0.5175, 0.5494,  ..., 0.4691, 0.4977, 0.5521],\n",
      "         [0.5165, 0.3864, 0.5060,  ..., 0.4621, 0.3481, 0.5995],\n",
      "         [0.6963, 0.5445, 0.5476,  ..., 0.4102, 0.3695, 0.5724],\n",
      "         ...,\n",
      "         [0.4519, 0.5980, 0.5177,  ..., 0.3988, 0.4858, 0.5163],\n",
      "         [0.4597, 0.5440, 0.5137,  ..., 0.6468, 0.4916, 0.5625],\n",
      "         [0.4370, 0.5559, 0.5096,  ..., 0.5141, 0.5751, 0.6369]],\n",
      "\n",
      "        [[0.5295, 0.4176, 0.4196,  ..., 0.4385, 0.4838, 0.4923],\n",
      "         [0.4632, 0.3816, 0.5374,  ..., 0.3701, 0.4470, 0.3635],\n",
      "         [0.3701, 0.2900, 0.4889,  ..., 0.7108, 0.3359, 0.4454],\n",
      "         ...,\n",
      "         [0.3479, 0.2867, 0.4490,  ..., 0.4289, 0.5667, 0.4624],\n",
      "         [0.4036, 0.2990, 0.3204,  ..., 0.3261, 0.4693, 0.4683],\n",
      "         [0.4847, 0.4259, 0.3938,  ..., 0.4731, 0.5386, 0.3858]],\n",
      "\n",
      "        [[0.3999, 0.5577, 0.5652,  ..., 0.5973, 0.5363, 0.5518],\n",
      "         [0.4807, 0.6005, 0.6339,  ..., 0.6455, 0.5104, 0.5780],\n",
      "         [0.5172, 0.5760, 0.5934,  ..., 0.6622, 0.6349, 0.6360],\n",
      "         ...,\n",
      "         [0.4831, 0.5916, 0.6770,  ..., 0.5796, 0.4507, 0.5105],\n",
      "         [0.4958, 0.5024, 0.5549,  ..., 0.5417, 0.5736, 0.4474],\n",
      "         [0.5660, 0.4919, 0.6384,  ..., 0.4913, 0.4753, 0.4653]],\n",
      "\n",
      "        [[0.5040, 0.5135, 0.5196,  ..., 0.5562, 0.4095, 0.5253],\n",
      "         [0.4922, 0.6048, 0.5529,  ..., 0.7029, 0.4942, 0.5169],\n",
      "         [0.4379, 0.5511, 0.4477,  ..., 0.3905, 0.4125, 0.4750],\n",
      "         ...,\n",
      "         [0.4807, 0.7427, 0.6543,  ..., 0.5945, 0.4223, 0.5557],\n",
      "         [0.5640, 0.5634, 0.5195,  ..., 0.5247, 0.6352, 0.5551],\n",
      "         [0.5248, 0.4830, 0.4674,  ..., 0.4659, 0.4464, 0.4713]],\n",
      "\n",
      "        [[0.5927, 0.4758, 0.6377,  ..., 0.4264, 0.5681, 0.5871],\n",
      "         [0.5649, 0.6043, 0.4673,  ..., 0.5576, 0.4600, 0.6416],\n",
      "         [0.5909, 0.4904, 0.6041,  ..., 0.5959, 0.4902, 0.6035],\n",
      "         ...,\n",
      "         [0.6045, 0.4738, 0.6108,  ..., 0.4428, 0.6587, 0.5517],\n",
      "         [0.4250, 0.4765, 0.6200,  ..., 0.4878, 0.5062, 0.4884],\n",
      "         [0.6470, 0.5862, 0.5349,  ..., 0.5121, 0.6078, 0.4595]]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, x, \"model.onnx\")\n",
    "print(out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代数据集\n",
    "batch_size = 16\n",
    "# data_loader = build_dataset(yaml_path=\"/home/szt/projects/ultralytics/urpc2020.yaml\", batch_size=batch_size, data_transform=None)\n",
    "data_loader = build_dataset(batch_size=batch_size, data_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from yolov8\n",
    "\n",
    "import numpy as np\n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): the bounding boxes to clip\n",
    "        shape (tuple): the shape of the image\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor | numpy.ndarray): Clipped boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n",
    "        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1\n",
    "        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1\n",
    "        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2\n",
    "        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "    return boxes\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    dw = x[..., 2] / 2  # half-width\n",
    "    dh = x[..., 3] / 2  # half-height\n",
    "    y[..., 0] = x[..., 0] - dw  # top left x\n",
    "    y[..., 1] = x[..., 1] - dh  # top left y\n",
    "    y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y,\n",
    "    width and height are normalized to image dimensions.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "        w (int): The width of the image. Defaults to 640\n",
    "        h (int): The height of the image. Defaults to 640\n",
    "        clip (bool): If True, the boxes will be clipped to the image boundaries. Defaults to False\n",
    "        eps (float): The minimum value of the box's width and height. Defaults to 0.0\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height, normalized) format\n",
    "    \"\"\"\n",
    "    if clip:\n",
    "        x = clip_boxes(x, (h - eps, w - eps))\n",
    "    assert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bbox_iou(bbox1, bbox2, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two bounding boxes.\n",
    "    :param bbox1: bounding box No.1.\n",
    "    :param bbox2: bounding box No.2.\n",
    "    :return: IoU of bbox1 and bbox2.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1, _ = bbox1\n",
    "    xmin1, ymin1 = x1 - w1 / 2.0, y1 - h1 / 2.0\n",
    "    xmax1, ymax1 = x1 + w1 / 2.0, y1 + h1 / 2.0\n",
    "    x2, y2, w2, h2, _ = bbox2\n",
    "    xmin2, ymin2 = x2 - w2 / 2.0, y2 - h2 / 2.0\n",
    "    xmax2, ymax2 = x2 + w2 / 2.0, y2 + h2 / 2.0\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    xx1 = np.max([xmin1, xmin2])\n",
    "    yy1 = np.max([ymin1, ymin2])\n",
    "    xx2 = np.min([xmax1, xmax2])\n",
    "    yy2 = np.min([ymax1, ymax2])\n",
    "\n",
    "    # Calculate intersection area\n",
    "    w = np.max([0.0, xx2 - xx1])\n",
    "    h = np.max([0.0, yy2 - yy1])\n",
    "    area_intersection = w * h\n",
    "\n",
    "    # Calculate union area (subtract overlapping area to avoid double counting)\n",
    "    area1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    area2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "    area_union = area1 + area2 - area_intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = area_intersection / (area_union + eps)\n",
    "    return iou\n",
    "\n",
    "def bboxes_iou(bbox1, bboxes):\n",
    "    # return iou of bbox1 and bboxes\n",
    "    ious = []\n",
    "    for bbox2 in bboxes:\n",
    "        iou = bbox_iou(bbox1.detach().numpy(), bbox2.squeeze(0).detach().numpy())\n",
    "        ious.append(iou)\n",
    "    return torch.tensor(ious)\n",
    "\n",
    "\n",
    "\n",
    "# loss\n",
    "box_loss = nn.MSELoss()\n",
    "cls_loss = nn.CrossEntropyLoss()\n",
    "def calculate_loss(pred_bbox, pred_cls, gt_bboxes, gt_clss):\n",
    "    b_loss = box_loss(pred_bbox, gt_bboxes)\n",
    "    c_loss = cls_loss(pred_cls, gt_clss)\n",
    "    loss = b_loss + c_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# assigner\n",
    "def pred_gt_assigner(pred_boxes, gt_boxes, iou_threshold=0.5, max_num_negatives=3):\n",
    "\n",
    "    h = len(gt_boxes)\n",
    "    w = pred_boxes.shape[0]\n",
    "    assgined_dict = {}\n",
    "    for i in range(h):\n",
    "        assgined_dict[i] = []\n",
    "\n",
    "    # 遍历每一个真实标签框  \n",
    "    for i, gt_box in enumerate(gt_boxes):  \n",
    "        \n",
    "        # 遍历每一个预测框  \n",
    "        for j, pred_box in enumerate(pred_boxes):  \n",
    "            # 计算IoU  \n",
    "            iou = bbox_iou(gt_box, pred_box)\n",
    "            if(iou > iou_threshold):\n",
    "                assgined_dict[i].append(j)\n",
    "    \n",
    "    return assgined_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [02:37<2:08:49, 157.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0, time:  157.7312 , loss:   0.1932  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [05:14<2:05:42, 157.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, time:  156.7197 , loss:   0.0797  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [07:50<2:02:43, 156.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    2, time:  156.1036 , loss:   0.0458  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [10:26<1:59:56, 156.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    3, time:  156.1086 , loss:   0.0326  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [13:02<1:57:11, 156.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    4, time:  155.8935 , loss:   0.0257  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [15:38<1:54:23, 156.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    5, time:  155.5101 , loss:   0.0215  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [18:15<1:52:04, 156.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    6, time:  157.1736 , loss:   0.0188  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [20:51<1:49:26, 156.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    7, time:  156.2668 , loss:   0.0171  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [23:27<1:46:46, 156.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    8, time:  156.0673 , loss:   0.0157  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [26:03<1:44:03, 156.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    9, time:  155.6654 , loss:   0.0144  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [28:39<1:41:30, 156.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   10, time:  156.3595 , loss:   0.0137  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [31:15<1:38:49, 156.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   11, time:  155.7436 , loss:   0.0129  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [33:51<1:36:14, 156.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   12, time:  156.1160 , loss:   0.0122  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [36:27<1:33:34, 155.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   13, time:  155.7293 , loss:   0.0121  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [39:03<1:31:00, 156.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   14, time:  156.1810 , loss:   0.0113  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [41:40<1:28:32, 156.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   15, time:  156.8014 , loss:   0.0113  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [44:16<1:26:01, 156.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   16, time:  156.7053 , loss:   0.0107  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [46:52<1:23:21, 156.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   17, time:  156.0349 , loss:   0.0106  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [49:29<1:20:44, 156.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   18, time:  156.1906 , loss:   0.0101  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [52:05<1:18:12, 156.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   19, time:  156.7884 , loss:   0.0101  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [54:42<1:15:36, 156.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   20, time:  156.4897 , loss:   0.0098  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [57:19<1:13:02, 156.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   21, time:  156.7089 , loss:   0.0102  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [59:55<1:10:26, 156.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   22, time:  156.5649 , loss:   0.0094  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [1:02:31<1:07:47, 156.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   23, time:  156.2857 , loss:   0.0093  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [1:05:08<1:05:13, 156.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   24, time:  156.7170 , loss:   0.0092  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [1:07:45<1:02:36, 156.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   25, time:  156.4173 , loss:   0.0091  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [1:10:20<59:54, 156.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   26, time:  155.7537 , loss:   0.0089  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [1:12:57<57:17, 156.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   27, time:  156.1893 , loss:   0.0089  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [1:15:33<54:41, 156.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   28, time:  156.3384 , loss:   0.0088  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [1:18:09<52:04, 156.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   29, time:  156.0315 , loss:   0.0086  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [1:20:45<49:29, 156.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   30, time:  156.4908 , loss:   0.0085  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [1:23:22<46:56, 156.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   31, time:  156.8681 , loss:   0.0085  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [1:25:59<44:20, 156.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   32, time:  156.5886 , loss:   0.0084  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [1:28:36<41:45, 156.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   33, time:  156.8112 , loss:   0.0084  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [1:31:12<39:09, 156.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   34, time:  156.6352 , loss:   0.0082  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [1:33:48<36:28, 156.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   35, time:  155.6748 , loss:   0.0082  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [1:36:24<33:52, 156.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   36, time:  156.2765 , loss:   0.0081  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [1:39:01<31:16, 156.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   37, time:  156.5742 , loss:   0.0081  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [1:41:38<28:41, 156.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   38, time:  156.7960 , loss:   0.0081  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [1:44:14<26:03, 156.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   39, time:  156.0383 , loss:   0.0080  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [1:46:50<23:27, 156.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   40, time:  156.2728 , loss:   0.0079  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [1:49:26<20:49, 156.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   41, time:  155.9099 , loss:   0.0079  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [1:52:02<18:12, 156.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   42, time:  155.8130 , loss:   0.0078  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [1:54:39<15:38, 156.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   43, time:  157.1809 , loss:   0.0079  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [1:57:16<13:02, 156.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   44, time:  156.9342 , loss:   0.0079  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [1:59:52<10:26, 156.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   45, time:  156.4235 , loss:   0.0077  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [2:02:29<07:49, 156.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   46, time:  156.4918 , loss:   0.0077  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [2:05:06<05:13, 156.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   47, time:  156.7934 , loss:   0.0076  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [2:07:42<02:36, 156.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   48, time:  156.0372 , loss:   0.0076  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [2:10:19<00:00, 156.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   49, time:  157.1928 , loss:   0.0076  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.ops import nms\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, weight_decay=0.005)  # 使用随机梯度下降优化器  \n",
    "epochs = 50  # 训练100轮\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss_t = []\n",
    "    for e, batch in enumerate(data_loader):\n",
    "        features, labels = batch\n",
    "        b_t = features.shape[0]\n",
    "        labels = group_adjust(labels, b_t) # b, n, 5; list of tensor\n",
    "        if(len(labels) < b_t):\n",
    "            continue\n",
    "        \n",
    "        # b, 5, 20, 20; b, 4, 20, 20\n",
    "        pred_boxes, pred_classes = model(features) # first! consider output box as center and width, height\n",
    "        pred_boxes = pred_boxes.permute(0, 2, 3, 1)\n",
    "        pred_classes = pred_classes.permute(0, 2, 3, 1)\n",
    "\n",
    "        # b, 20, 20, 5; b, 20, 20, 4\n",
    "        gt_boxes = torch.zeros(b_t, 20, 20, 5)\n",
    "        gt_classes = torch.zeros(b_t, 20, 20, 4)\n",
    "        for i, label in enumerate(labels):\n",
    "            for j, lable_ in enumerate(label):\n",
    "                x_idx, y_idx = int(lable_[1] * 20), int(lable_[2] * 20)\n",
    "                x_idx = np.clip(x_idx, 0, 19)\n",
    "                y_idx = np.clip(y_idx, 0, 19)\n",
    "                gt_boxes[i, x_idx, y_idx, 0] = lable_[1] * 20.0 - x_idx\n",
    "                gt_boxes[i, x_idx, y_idx, 1] = lable_[2] * 20.0 - y_idx\n",
    "                gt_boxes[i, x_idx, y_idx, 2:4] = lable_[3:]\n",
    "                gt_boxes[i, x_idx, y_idx, 4] = 1\n",
    "                gt_classes[i, x_idx, y_idx, int(lable_[0])] = 1\n",
    "        \n",
    "        have_obg = (gt_boxes[..., 4] == 1)\n",
    "        no_obj = ~have_obg\n",
    "\n",
    "        # print(gt_boxes.shape, gt_classes.shape)\n",
    "        # print(have_obg.shape)\n",
    "\n",
    "        # print(pred_boxes[0], gt_boxes[0])\n",
    "        loss_coor = ((gt_boxes[..., :2] - pred_boxes[..., :2]) ** 2 \\\n",
    "                    + (torch.sqrt(pred_boxes[..., 2:4]) - torch.sqrt(gt_boxes[..., 2:4])) ** 2).sum(dim=-1) * have_obg\n",
    "        \n",
    "        loss_confidence = (gt_boxes[..., 4] - pred_boxes[..., 4]) ** 2\n",
    "        # print(loss_coor.shape)\n",
    "\n",
    "        loss_class = ((pred_classes - gt_classes) ** 2).sum(dim=-1) * have_obg\n",
    "        # print(loss_class.shape)\n",
    "        \n",
    "        loss_noOb = (gt_boxes[..., 4] - pred_boxes[..., 4]) ** 2 * no_obj\n",
    "\n",
    "        loss = (1.0 * loss_coor + loss_confidence + loss_class + 0.1 * loss_noOb).mean()\n",
    "\n",
    "        loss_t.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"epoch: {epoch:4}, time: {time:^10.4f}, loss: {loss:^10.4f}\".format(epoch=epoch, time=end_time-start_time, loss=np.mean(loss_t)))\n",
    "\n",
    "# bbox1 = torch.tensor([100, 100, 50, 50])\n",
    "# bbox2 = torch.tensor([75, 75, 50, 50])\n",
    "# iou = bbox_iou(bbox1, bbox2)\n",
    "# print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"hg_model.pth\") # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 20]) torch.Size([4, 20, 20])\n",
      "torch.Size([400, 1]) torch.Size([400, 5])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = Image.open(\"/home/szt/datasets/hg_underwater_target/纯色曝光1000us距离1m正常水下不同角度位置橙色背景/1702521983.607.24.jpeg\").convert(\"RGB\")\n",
    "original_img = cv2.imread(\"/home/szt/datasets/hg_underwater_target/纯色曝光1000us距离1m正常水下不同角度位置橙色背景/1702521983.607.24.jpeg\", cv2.COLOR_BGR2RGB)\n",
    "original_img = cv2.resize(original_img, (640, 640))\n",
    "data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Resize((640, 640), antialias=True), # image resize but label is ratio, so label is not need to change\n",
    "                                            ])\n",
    "model = torch.load(\"./hg_model.pth\")\n",
    "img = data_transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "out = model(img)\n",
    "bboxes, clss = out\n",
    "\n",
    "bt = bboxes.shape[0]\n",
    "print(bboxes[0].shape, clss[0].shape)\n",
    "box = bboxes[0].permute(1,2,0)\n",
    "cs = clss[0].permute(1,2,0)\n",
    "\n",
    "# 生成预测框\n",
    "pred_boxes = torch.zeros_like(box)\n",
    "pred_score = torch.zeros(20, 20, 1)\n",
    "pred_class = torch.zeros(20, 20, 1)\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        x, y, w, h, confidence = (box[i][j][0] + i) * 640, (box[i][j][1] + j) / 20.0 * 640, box[i][j][2] * 640, box[i][j][3] * 640, box[i][j][4]\n",
    "        pred_boxes[i, j, :] = torch.stack([x, y, w, h, confidence])\n",
    "        score, cata = cs[i][j].max(dim=-1)\n",
    "        pred_class[i, j, :] = cata\n",
    "        pred_score[i, j, :] = score\n",
    "\n",
    "pred_boxes = pred_boxes.view(-1, 5)\n",
    "pred_class = pred_class.view(-1, 1)\n",
    "pred_score = pred_score.view(-1, 1)\n",
    "print(pred_class.shape, pred_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nms, non-maximum suppression\n",
    "def nms_(pred_bbox, iou_threshold=0.5, score_threshold=0.3):\n",
    "    # pred_bbox: h*w, num_anchors*4\n",
    "    # pred_cls: h*w\n",
    "    indices = []\n",
    "    \n",
    "    # according to cls scores, sort the bbox\n",
    "    confidence = pred_bbox[..., 4]\n",
    "    # print(confidence.shape)\n",
    "    sorted_indices = torch.argsort(confidence, descending=True)\n",
    "    # print(sorted_indices)\n",
    "    while sorted_indices.size(0) > 0:\n",
    "        i = sorted_indices[0]\n",
    "        if(confidence[i] < score_threshold): break\n",
    "        indices.append(i)\n",
    "        if sorted_indices.size(0) == 1:\n",
    "            break\n",
    "        iou = bboxes_iou(pred_bbox[i].squeeze(0), pred_bbox[sorted_indices[1:]].squeeze(0))\n",
    "        sorted_indices = sorted_indices[1:][iou < iou_threshold]\n",
    "    # \n",
    "    return torch.stack(indices)\n",
    "\n",
    "# 执行NMS\n",
    "keep_indices = nms_(pred_boxes, 0.5, 0.05)\n",
    "# print(keep_indices)\n",
    "# print(pred_boxes[keep_indices], pred_class[keep_indices], pred_score[keep_indices])\n",
    "\n",
    "# keep_indices中存储了保留的预测框的索引\n",
    "# 可以根据这些索引获取最终的目标框\n",
    "# filer score\n",
    "for box, s in zip(pred_boxes[keep_indices], pred_class[keep_indices]):\n",
    "    cv2.putText(original_img, str(int(s.item())), (int(box[0] - box[2] / 2), int(box[1] - box[3] / 2)), cv2.FONT_ITALIC, 1.0, (0, 255, 0), 1)\n",
    "    cv2.rectangle(original_img, (int(box[0] - box[2] / 2), int(box[1] - box[3] / 2)), (int(box[0] + box[2] / 2), int(box[1] + box[3] / 2)), (0, 255, 0), 1)\n",
    "cv2.imwrite(\"test.jpg\", original_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "original_img = cv2.imread(\"/home/szt/datasets/hg_underwater_target/hard_samples_test/1702521478.434.3239.jpeg\", cv2.COLOR_BGR2RGB)\n",
    "original_img = cv2.resize(original_img, (640, 640))\n",
    "\n",
    "for i, (imgs, labels) in enumerate(data_loader):\n",
    "    b_t = features.shape[0]\n",
    "    labels = group_adjust(labels, b_t) # b, n, 5; list of tensor\n",
    "    if(len(labels) < b_t):\n",
    "        continue\n",
    "    \n",
    "    # b, 5, 20, 20; b, 4, 20, 20\n",
    "    # pred_boxes, pred_classes = model(features) # first! consider output box as center and width, height\n",
    "    img = imgs[0].permute(1,2,0)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    print(img.shape)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for j, lable_ in enumerate(labels[0]):\n",
    "        (c, x, y, w, h) = lable_\n",
    "        minx, miny = x - w / 2, y - h / 2\n",
    "        maxx, maxy = x + w / 2, y + h / 2\n",
    "        \n",
    "        minx, miny = minx.item() * 640, miny.item() * 640\n",
    "        maxx, maxy = maxx.item() * 640, maxy.item() * 640\n",
    "        cv2.rectangle(img, (int(minx), int(miny)), (int(maxx), int(maxy)), (255, 255, 0), 1)\n",
    "\n",
    "    cv2.imwrite(\"test.jpg\", img)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
